{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMPF Equivalency Pattern Recognition Engine - FIXED\n",
    "\n",
    "## From Months to Minutes: The Leibniz I-Ching Indra's Net Conjecture\n",
    "\n",
    "**Fixed version** that resolves Kaggle dependency conflicts and uses compatible package versions.\n",
    "\n",
    "### Key Changes:\n",
    "- Compatible transformers/trl versions\n",
    "- Alternative training approach if SFTTrainer fails\n",
    "- Robust error handling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup - FIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install compatible versions for Kaggle\n",
    "!pip install torch torchvision torchaudio --quiet\n",
    "!pip install transformers>=4.41.0 --quiet\n",
    "!pip install datasets>=2.15.0 --quiet\n",
    "!pip install accelerate>=0.25.0 --quiet\n",
    "\n",
    "# Try to install TRL, fallback if it fails\n",
    "try:\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"trl>=0.7.4\", \"--quiet\"], check=True)\n",
    "    TRL_AVAILABLE = True\n",
    "    print(\"âœ“ TRL installed successfully\")\nexcept Exception as e:\n",
    "    TRL_AVAILABLE = False\n",
    "    print(f\"âš  TRL installation failed: {e}\")\n",
    "    print(\"Will use standard Trainer instead\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries with fallbacks\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# Try to import SFTTrainer, use fallback if not available\n",
    "try:\n",
    "    from trl import SFTTrainer\n",
    "    print(\"âœ“ SFTTrainer imported successfully\")\n",
    "    USE_SFT = True\nexcept ImportError:\n",
    "    print(\"âš  SFTTrainer not available, using standard Trainer\")\n",
    "    USE_SFT = False\n",
    "\n",
    "import gc\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Check GPU\n",
    "print(f\"\\nGPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "else:\n",
    "    print(\"Using CPU - training will be slower\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load UMPF Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the equivalency training dataset\n",
    "# Make sure to upload equivalency-training-pairs.json as a dataset\n",
    "\n",
    "# Try different possible paths\n",
    "possible_paths = [\n",
    "    \"/kaggle/input/umpf-equivalency-training/equivalency-training-pairs.json\",  # Updated path\n",
    "    \"/kaggle/input/umpf-training/equivalency-training-pairs.json\",\n",
    "    \"/kaggle/working/equivalency-training-pairs.json\",\n",
    "    \"../input/equivalency-training-pairs.json\",\n",
    "    \"./equivalency-training-pairs.json\"\n",
    "]\n",
    "\n",
    "training_data_path = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        training_data_path = path\n",
    "        print(f\"âœ“ Found training data at: {path}\")\n",
    "        break\n",
    "\n",
    "if not training_data_path:\n",
    "    raise FileNotFoundError(\"Please upload equivalency-training-pairs.json as a dataset named 'umpf-equivalency-training'\")\n",
    "\n",
    "# Load the data\n",
    "with open(training_data_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "training_examples = data[\"equivalency_training_dataset\"][\"training_examples\"]\n",
    "system_prompt = data[\"equivalency_training_dataset\"][\"system_prompt\"]\n",
    "\n",
    "print(f\"Loaded {len(training_examples)} training examples\")\n",
    "print(f\"System prompt: {system_prompt[:100]}...\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing - Enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Enhanced data formatting with multiple approaches\n",
    "def format_training_examples_sft(training_examples):\n",
    "    \"\"\"Format for SFTTrainer\"\"\"\n",
    "    formatted_examples = []\n",
    "    \n",
    "    for example in training_examples:\n",
    "        messages = example[\"messages\"]\n",
    "        \n",
    "        # Combine system, user, and assistant messages\n",
    "        conversation = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                conversation += f\"<|system|>{msg['content']}<|endoftext|>\"\n",
    "            elif msg[\"role\"] == \"user\":\n",
    "                conversation += f\"<|user|>{msg['content']}<|endoftext|>\"\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                conversation += f\"<|assistant|>{msg['content']}<|endoftext|>\"\n",
    "        \n",
    "        formatted_examples.append({\"text\": conversation})\n",
    "    \n",
    "    return formatted_examples\n",
    "\n",
    "def format_training_examples_standard(training_examples):\n",
    "    \"\"\"Format for standard Trainer\"\"\"\n",
    "    formatted_examples = []\n",
    "    \n",
    "    for example in training_examples:\n",
    "        messages = example[\"messages\"]\n",
    "        \n",
    "        # Extract user prompt and assistant response\n",
    "        user_msg = next((msg['content'] for msg in messages if msg['role'] == 'user'), \"\")\n",
    "        assistant_msg = next((msg['content'] for msg in messages if msg['role'] == 'assistant'), \"\")\n",
    "        \n",
    "        # Create input-output pair\n",
    "        text = f\"Human: {user_msg}\\n\\nAssistant: {assistant_msg}\"\n",
    "        formatted_examples.append({\"text\": text})\n",
    "    \n",
    "    return formatted_examples\n",
    "\n",
    "# Choose formatting based on available trainer\n",
    "if USE_SFT:\n",
    "    formatted_examples = format_training_examples_sft(training_examples)\n",
    "    print(\"Using SFTTrainer formatting\")\nelse:\n",
    "    formatted_examples = format_training_examples_standard(training_examples)\n",
    "    print(\"Using standard Trainer formatting\")\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = Dataset.from_list(formatted_examples)\n",
    "print(f\"Created dataset with {len(train_dataset)} examples\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\n=== Sample Training Example ===\")\n",
    "print(train_dataset[0]['text'][:500] + \"...\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Setup - Robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model configuration for Kaggle with fallbacks\n",
    "MODEL_OPTIONS = [\n",
    "    \"microsoft/DialoGPT-medium\",\n",
    "    \"gpt2-medium\",\n",
    "    \"gpt2\"\n",
    "]\n",
    "\n",
    "MAX_LENGTH = 1024  # Conservative for memory\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "model_name = None\n",
    "\n",
    "# Try models in order of preference\n",
    "for model_candidate in MODEL_OPTIONS:\n",
    "    try:\n",
    "        print(f\"Trying to load model: {model_candidate}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_candidate)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_candidate,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        \n",
    "        model_name = model_candidate\n",
    "        print(f\"âœ“ Successfully loaded: {model_candidate}\")\n",
    "        break\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Failed to load {model_candidate}: {e}\")\n",
    "        continue\n",
    "\n",
    "if model is None:\n",
    "    raise RuntimeError(\"Could not load any model\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Using model: {model_name}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Max sequence length: {MAX_LENGTH}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Configuration - Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training arguments optimized for Kaggle\n",
    "output_dir = \"/kaggle/working/umpf-equivalency-model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Conservative settings for stability\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=2,  # Reduced for faster training\n",
    "    per_device_train_batch_size=1,  # Very conservative\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=10,\n",
    "    learning_rate=5e-6,  # Lower learning rate\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    load_best_model_at_end=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=0,\n",
    "    report_to=None,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    prediction_loss_only=True,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  FP16: {training_args.fp16}\")\n",
    "print(f\"  Gradient checkpointing: {training_args.gradient_checkpointing}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tokenization - Robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "# Add labels for language modeling\n",
    "tokenized_dataset = tokenized_dataset.map(\n",
    "    lambda examples: {\"labels\": examples[\"input_ids\"]}\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Dataset tokenized: {len(tokenized_dataset)} examples\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Start Training - Flexible Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal language modeling\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Starting UMPF Equivalency Training...\")\n",
    "print(\"This will train the model to recognize universal computational patterns!\")\n",
    "\n",
    "# Try SFTTrainer first, fallback to standard Trainer\n",
    "trainer = None\n",
    "training_successful = False\n",
    "\n",
    "if USE_SFT:\n",
    "    try:\n",
    "        print(\"Attempting SFTTrainer...\")\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            train_dataset=train_dataset,  # Use original dataset\n",
    "            tokenizer=tokenizer,\n",
    "            args=training_args,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=MAX_LENGTH,\n",
    "        )\n",
    "        trainer.train()\n",
    "        training_successful = True\n",
    "        print(\"âœ“ SFTTrainer succeeded\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— SFTTrainer failed: {e}\")\n",
    "        print(\"Falling back to standard Trainer...\")\n",
    "\n",
    "if not training_successful:\n",
    "    try:\n",
    "        print(\"Using standard Trainer...\")\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        trainer.train()\n",
    "        training_successful = True\n",
    "        print(\"âœ“ Standard Trainer succeeded\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Standard Trainer failed: {e}\")\n",
    "        raise RuntimeError(\"All training approaches failed\")\n",
    "\n",
    "if training_successful:\n",
    "    print(\"\\nâœ… Training completed successfully!\")\nelse:\n",
    "    raise RuntimeError(\"Training failed\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the trained model\n",
    "print(f\"Saving model to {output_dir}\")\n",
    "\n",
    "try:\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(\"âœ… Model saved successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")\n",
    "    # Manual save as fallback\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(\"âœ… Model saved manually!\")\n",
    "\n",
    "# Clean up GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"GPU memory cleaned up\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the trained model's equivalency generation\n",
    "print(\"ðŸ§ª Testing UMPF Equivalency Generation...\")\n",
    "\n",
    "try:\n",
    "    # Load model for inference\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=output_dir,\n",
    "        tokenizer=output_dir,\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    )\n",
    "\n",
    "    # Test prompts\n",
    "    test_prompts = [\n",
    "        \"Generate an equivalency pair for atomic-level uncertainty patterns.\",\n",
    "        \"Generate an equivalency pair for database transactions and musical improvisation.\",\n",
    "    ]\n",
    "\n",
    "    system_msg = \"You are a Universal Pattern Recognition Engine trained on the Leibniz I-Ching Indra's Net Conjecture.\"\n",
    "\n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        print(f\"\\n=== Test {i}: {prompt} ===\")\n",
    "        \n",
    "        # Adjust prompt format based on training method\n",
    "        if USE_SFT:\n",
    "            full_prompt = f\"<|system|>{system_msg}<|endoftext|><|user|>{prompt}<|endoftext|><|assistant|>\"\n",
    "        else:\n",
    "            full_prompt = f\"Human: {prompt}\\n\\nAssistant:\"\n",
    "        \n",
    "        try:\n",
    "            response = generator(\n",
    "                full_prompt,\n",
    "                max_length=len(tokenizer.encode(full_prompt)) + 200,  # Dynamic length\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            \n",
    "            generated_text = response[0][\"generated_text\"]\n",
    "            \n",
    "            # Extract assistant's response\n",
    "            if USE_SFT:\n",
    "                assistant_response = generated_text.split(\"<|assistant|>\")[-1].strip()\n",
    "            else:\n",
    "                assistant_response = generated_text.split(\"Assistant:\")[-1].strip()\n",
    "            \n",
    "            print(assistant_response[:300] + \"...\" if len(assistant_response) > 300 else assistant_response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "\n",
    "    print(\"\\nðŸŽ‰ UMPF Equivalency Pattern Recognition Engine is ready!\")\n",
    "    print(\"The model can now automatically discover computational equivalencies!\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"Error during testing: {e}\")\n",
    "    print(\"Model training completed but testing failed\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\nðŸ“ˆ UMPF Training Summary:\")\nprint(\"=\" * 50)\nprint(f\"âœ“ Model: {model_name}\")\nprint(f\"âœ“ Training examples: {len(training_examples)}\")\nprint(f\"âœ“ Training method: {'SFTTrainer' if USE_SFT else 'Standard Trainer'}\")\nprint(f\"âœ“ Epochs: {training_args.num_train_epochs}\")\nprint(f\"âœ“ Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"âœ“ Learning rate: {training_args.learning_rate}\")\nprint(f\"âœ“ Model saved to: {output_dir}\")\n\nprint(\"\\nðŸŽ¯ Next Steps:\")\nprint(\"  â†’ Model is ready for equivalency pattern generation\")\nprint(\"  â†’ Can identify universal computational patterns across domains\")\nprint(\"  â†’ Implements the Leibniz I-Ching Indra's Net Conjecture\")\nprint(\"  â†’ Transforms research time from months to minutes!\")\n\nprint(\"\\nðŸ”¬ Scientific Impact:\")\nprint(\"  â€¢ Automated discovery of cross-domain equivalencies\")\nprint(\"  â€¢ Pattern-based hypothesis generation\")\nprint(\"  â€¢ Universal computational structure recognition\")\nprint(\"  â€¢ Monadic algebra for scientific automation\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}